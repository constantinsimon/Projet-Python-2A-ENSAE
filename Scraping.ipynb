{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a38200fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (2.32.5)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: bs4 in ./.venv/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.12/site-packages (6.0.2)\n",
      "Requirement already satisfied: html5lib in ./.venv/lib/python3.12/site-packages (1.1)\n",
      "Collecting geopy\n",
      "  Downloading geopy-2.4.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.12/site-packages (from bs4) (4.14.3)\n",
      "Requirement already satisfied: six>=1.9 in ./.venv/lib/python3.12/site-packages (from html5lib) (1.17.0)\n",
      "Requirement already satisfied: webencodings in ./.venv/lib/python3.12/site-packages (from html5lib) (0.5.1)\n",
      "Collecting geographiclib<3,>=1.52 (from geopy)\n",
      "  Downloading geographiclib-2.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in ./.venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (4.15.0)\n",
      "Downloading geopy-2.4.1-py3-none-any.whl (125 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scikit_learn-1.7.2-cp312-cp312-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Downloading geographiclib-2.1-py3-none-any.whl (40 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, geographiclib, scikit-learn, geopy\n",
      "Successfully installed geographiclib-2.1 geopy-2.4.1 joblib-1.5.2 scikit-learn-1.7.2 scipy-1.16.3 threadpoolctl-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests pandas matplotlib seaborn bs4 lxml html5lib geopy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b282510d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://fr.wikipedia.org/wiki/Liste_des_gares_desservies_par_TGV\"\n",
    "\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "tables = soup.find_all(\"table\", {\"class\": \"wikitable\"})\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for table in tables:\n",
    "    headers = [th.text.strip() for th in table.find_all(\"th\")]\n",
    "    \n",
    "    for row in table.find_all(\"tr\")[1:]:\n",
    "        cells = [td.text.strip() for td in row.find_all([\"td\", \"th\"])]\n",
    "        if len(cells) == len(headers):\n",
    "            all_rows.append(dict(zip(headers, cells)))\n",
    "\n",
    "df = pd.DataFrame(all_rows)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e088de12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page récupérée avec succès !\n",
      "------------------------------\n",
      "Tableau trouvé avec 182 gares !\n",
      "                               Gare        Département No dpt  \\\n",
      "0  Aéroport Charles-de-Gaulle 2 TGV  Seine-Saint-Denis     93   \n",
      "1                              Agde            Hérault     34   \n",
      "2                              Agen     Lot-et-Garonne     47   \n",
      "3                    Aime-La Plagne             Savoie     73   \n",
      "4               Aix-en-Provence TGV   Bouches-du-Rhône     13   \n",
      "\n",
      "                       Région  \\\n",
      "0               Île-de-France   \n",
      "1                   Occitanie   \n",
      "2          Nouvelle-Aquitaine   \n",
      "3        Auvergne-Rhône-Alpes   \n",
      "4  Provence-Alpes-Côte d'Azur   \n",
      "\n",
      "                                       Type de train  \\\n",
      "0                         TGV inOui, Ouigo, Eurostar   \n",
      "1                                   TGV inOui, Ouigo   \n",
      "2                                   TGV inOui, Ouigo   \n",
      "3  TGV inOui (saisonnier), Ouigo (saisonnier), Eu...   \n",
      "4  TGV inOui, TGV Lyria (saisonnier), Ouigo, Euro...   \n",
      "\n",
      "  Date de la première desserte  \n",
      "0             13 novembre 1994  \n",
      "1             11 décembre 1988  \n",
      "2            25 septembre 1990  \n",
      "3             11 décembre 1988  \n",
      "4                 10 juin 2001  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yassermouaqqat/Desktop/ENSAE/2A/S1/Python pour la data science/Projet-Python-2A-ENSAE/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'fr.wikipedia.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "\n",
    "url = \"https://fr.wikipedia.org/wiki/Liste_des_gares_desservies_par_TGV\"\n",
    "\n",
    "# 1. On se fait passer pour un navigateur web pour ne pas être bloqué\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # On fait la requête avec les headers et sans vérification SSL\n",
    "    response = requests.get(url, headers=headers, verify=False)\n",
    "    \n",
    "    # On vérifie si la page a bien été chargée (Code 200 = OK)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Page récupérée avec succès !\")\n",
    "        \n",
    "        # 2. On utilise io.StringIO pour que Pandas lise le texte comme un fichier\n",
    "        # 3. 'match=\"Département\"' force Pandas à ne garder que les tables qui contiennent ce mot\n",
    "        dfs = pd.read_html(io.StringIO(response.text), match=\"Département\")\n",
    "        \n",
    "        if len(dfs) > 0:\n",
    "            df_gares = dfs[0] # On prend la première table trouvée\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"Tableau trouvé avec {len(df_gares)} gares !\")\n",
    "            print(df_gares.head())\n",
    "        else:\n",
    "            print(\"Aucune table correspondante trouvée.\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Erreur : Wikipédia a renvoyé le code {response.status_code}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur s'est produite : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40ffea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
